{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Error Correction Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error correction module will take input from OCR and will try to match each word with the word present in the dictionary.   \n",
    "\n",
    "I have tried to to do that with three methods :  \n",
    "1. **Based on transformations:** The least number of transformations will be given the most preference.    \n",
    "2. **Based on the probability of the word:** Here, I have allows a 'Fuzzy window'/number of transformations as 2. So, for up to 2 transformations, it will try to calculate the combination of the word and its probability. And the top 3 most probable words will be presented.  \n",
    "3. **Using the N-gram Language model :** In the N-gram model I have calculated N-grams for the whole corpus, And for the test word, It will generate up to N-1 grams will generate their probabilities. From these probabilities, we can generate the approach of which character will be present next after the mistype.  \n",
    "\n",
    "\n",
    "\t   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting Imports : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Imports, I have used NLTK for Tokenizing and Collections for Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Handling :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FileLoad(filename):\n",
    "    file = open(filename, \"r\")\n",
    "    data = file.read()\n",
    "    file.close()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In FileLoad(), I am taking a file from Corpus and read the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "textInput = FileLoad(\"europarl-v7.bg-en.en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I have taken a file from Europarl corpus 'europarl-v7.bg-en.en'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(textInput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I am using NLTK's built-in function (word_tokenize()) which takes text as an argument and returns a list of tokenized words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [word.lower() for word in words if word.isalpha()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I am **preprocessing** the list of words by converting all the words into lowercase letters and only keeping the tokens which have alphabets in them like words, I have used word.isalpha() for this in order to remove the noise (. , / , \"\" etc..).  \n",
    "Here I have removed everything except words e.g. : Numbers and Punctuation.  \n",
    "But if we want to remove them separately, We can also use **isdigit()** for numbers or **re.sub()** for Regular expressions.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "StopW = set(stopwords.words(\"english\"))\n",
    "\n",
    "words = [w for w in words if not w in StopW]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I have used NLTK's **Stopwords** functionality to remove the stopwords such as the, is, at, which, on, etc. in order to further clean the list of tokens.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = [lemmatizer.lemmatize(w) for w in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I have used NLTK's Lemmatizers to only keep the root of the words in order to decrease the frequency of similar words.    \n",
    "\n",
    "**Stemming vs Lemmatization :**  \n",
    "In stemming, We remove the prefixes and suffixes to keep the stem of the word, even if the word is not valid.    \n",
    "This is done in order to keep a group of the same words with different grammar infliction together.      \n",
    "Generally, Stemmers have an algorithm so they are fast as compared to Lemmatizers.  \n",
    "In Lemmatizing, the algorithm makes sure that after the reduction is done, the word (root) still has a meaning/the word is present in the language.  \n",
    "\n",
    "Stemming vs Lemmatizing depends on the type of application.  \n",
    "As we are working with an OCR language application, language is important and so used lemmatization as it uses a corpus to match root forms.  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordDict = Counter(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counter to create a dictionary of words with frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "TotalWords = sum(WordDict.values())\n",
    "\n",
    "\n",
    "def probability(word):\n",
    "    prob = (WordDict[word] / TotalWords) * 100\n",
    "    return prob\n",
    "\n",
    "\n",
    "def edits1(word):\n",
    "    letters = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    deletes = [L + R[1:] for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "    replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
    "    inserts = [L + c + R for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "\n",
    "def edits2(word):\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I have defined 3 functions :   \n",
    "1. probability: Which calculates the probability of the word.  \n",
    "2. edits1: all the combination of words that are possible with 1 transformation  \n",
    "e.g.: splitting, deleting, transposing, replacing and inserting  \n",
    "3. edits2: all the combination of words that are possible with 2 transformations  \n",
    "\n",
    "Here the number of transformations represents the Edit distance between the two words.  \n",
    "Edit distance compares the two words and tries to quantify how dissimilar they are and how many operations it will take to match them.  \n",
    "\n",
    "Here I have kept maximum **Edit Distance of 2**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'layer' is the closest word in the corpus\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nOutput : \\n\\n'layer' is the closest word in the corpus\\n\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def suggestion1(word):\n",
    "    if word in WordDict:\n",
    "        #Checks if the word is present in the dictionary\n",
    "        print(\"Given word '{}' is Correct..!!!!\".format(word))\n",
    "    else:\n",
    "        #Edit distance = 1\n",
    "        firstpref = edits1(word)\n",
    "\n",
    "        for i in firstpref:\n",
    "            if i in WordDict:\n",
    "                prob = probability(i)\n",
    "                print(\n",
    "                    \"'{}' is the closest word in the corpus\".format(\n",
    "                        i\n",
    "                    )\n",
    "                )\n",
    "                return\n",
    "        \n",
    "        #Edit Distance = 2\n",
    "        secondpref = edits2(word)\n",
    "\n",
    "        for j in secondpref:\n",
    "            if j in WordDict:\n",
    "                prob = probability(j)\n",
    "                print(\n",
    "                    \"'{}' is the closest word in the corpus\".format(\n",
    "                        j\n",
    "                    )\n",
    "                )\n",
    "                return\n",
    "            else:\n",
    "                print(\"No word is closest to the given word - {}\".format(word))\n",
    "                return\n",
    "\n",
    "\n",
    "suggestion1(\"laywer\")\n",
    "\n",
    "\n",
    "'''\n",
    "Output : \n",
    "\n",
    "'layer' is the closest word in the corpus\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code block above, I have defined a function Suggestion1() which suggests the closest alternative to the given word.      \n",
    "The Function check if the given word is present in the dictionary. If not, Then it takes the Edit distance as 1 and tries to calculate the combination of words closest to the given word with 1 transformation.  \n",
    "And displays the word.    \n",
    "If there are no words in the dictionary with 1 transformation, Then we use Edits2 to make Edit Distance = 2 and make those words to check the closest one.    \n",
    "  \n",
    "In this function, We rely on the Error model on the terms of Edit Distance (If words in Edit distance = 1 dont match, Then go to Edit Distance = 2 or else no results).  \n",
    "  \n",
    "Here, Probability is not taken into account as each word at same edit distance will have the same p(w|c).  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 suggestions for the misinterprited word are :\n",
      " 1. 'later' with probability : 0.017776651158326995 % \n",
      " 2. 'lower' with probability : 0.013484339033537334 % \n",
      " 3. 'player' with probability : 0.013423598767620497 % \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nOutput : \\n\\nTop 3 suggestions for the misinterprited word are :\\n 1. 'later' with probability : 0.017776651158326995 % \\n 2. 'lower' with probability : 0.013484339033537334 % \\n 3. 'player' with probability : 0.013423598767620497 % \\n\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def suggestion2(word):\n",
    "    prob = {}\n",
    "    if word in WordDict:\n",
    "        print(\"Given word '{}' is Correct..!!!!\".format(word))\n",
    "    else:\n",
    "        firstpref = edits1(word)\n",
    "\n",
    "        for i in firstpref:\n",
    "            if i in WordDict:\n",
    "                prob[i] = probability(i)\n",
    "\n",
    "        secondpref = edits2(word)\n",
    "\n",
    "        for j in secondpref:\n",
    "            if j in WordDict:\n",
    "                prob[j] = probability(j)\n",
    "        # print(prob)\n",
    "\n",
    "    sortedSuggest = sorted(prob.items(), key=lambda x: x[1], reverse=True)\n",
    "    #Sorting in descending order of the probability\n",
    "\n",
    "    print(\n",
    "        \"Top 3 suggestions for the misinterprited word are :\\n\",\n",
    "        \"1. '{}' with probability : {} %\".format(\n",
    "            sortedSuggest[0][0], sortedSuggest[0][1]\n",
    "        ),\n",
    "        \"\\n\",\n",
    "        \"2. '{}' with probability : {} %\".format(\n",
    "            sortedSuggest[1][0], sortedSuggest[1][1]\n",
    "        ),\n",
    "        \"\\n\",\n",
    "        \"3. '{}' with probability : {} %\".format(\n",
    "            sortedSuggest[2][0], sortedSuggest[2][1]\n",
    "        ),\n",
    "        \"\\n\",\n",
    "    )\n",
    "\n",
    "\n",
    "suggestion2(\"laywer\")\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Output : \n",
    "\n",
    "Top 3 suggestions for the misinterprited word are :\n",
    " 1. 'later' with probability : 0.017776651158326995 % \n",
    " 2. 'lower' with probability : 0.013484339033537334 % \n",
    " 3. 'player' with probability : 0.013423598767620497 % \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Suggestion2(), We solely rely on the probability of the combination of words in a dictionary.  \n",
    "  \n",
    "In the function, We have allowed the fuzzy window of up to 2 transformations and calculated the probability of each word and displayed the top 3 suggestions with the highest probability.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing is found  at fictiona \n",
      "Probability of fiction is : 2.7109086965950985e-05 \n",
      "Probability of fictio is : 2.7109086965950985e-05 \n",
      "Probability of ficti is : 2.7109086965950985e-05 \n",
      "Probability of fict is : 2.7109086965950985e-05 \n",
      "Probability of fic is : 2.7109086965950985e-05 \n",
      "Probability of fi is : 4.744090219041423e-05 \n",
      "Probability of f is : 0.0001152136196052917 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nOutput : \\n\\nNothing is found  at fictiona \\nProbability of fiction is : 2.7109086965950985e-05 \\nProbability of fictio is : 2.7109086965950985e-05 \\nProbability of ficti is : 2.7109086965950985e-05 \\nProbability of fict is : 2.7109086965950985e-05 \\nProbability of fic is : 2.7109086965950985e-05 \\nProbability of fi is : 4.744090219041423e-05 \\nProbability of f is : 0.0001152136196052917 \\n\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents = \"Science Fantasy was a cacti playing British 3 fantasy and science fiction magazine, launched in 1950 by Nova Publications. 4 John Carnell edited the magazine beginning with the third issue, typically running a long lead novelette along with several shorter stories. Prominent contributors in the 1950s included John Brunner, Ken Bulmer, and Brian Aldiss, whose first novel Nonstop appeared (in an early version) in the February 1956 issue. Fantasy stories began to appear more frequently during the latter half of the 1950s, and in the early 1960s Carnell began to publish Thomas Burnett Swann's well-received historical fantasies. In the early 1960s Carnell's efforts were rewarded with three consecutive Hugo nominations for best magazine. After Nova went out of business in early 1964, Roberts & Vinter took over as publishers until 1967. Kyril Bonfiglioli, the editor, attracted new writers, including Keith Roberts, Brian Stableford and Josephine Saxton. In the opinion of science fiction historian Mike Ashley, the final year of the magazine, when it was renamed Impulse, included some of the best material ever published in a British science fiction magazine.\"\n",
    "\n",
    "words = word_tokenize(contents)\n",
    "\n",
    "words = [word.lower() for word in words if word.isalpha()]\n",
    "\n",
    "StopW = set(stopwords.words(\"english\"))\n",
    "\n",
    "words = [w for w in words if not w in StopW]\n",
    "\n",
    "# lemmatizing\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = [lemmatizer.lemmatize(w) for w in words]\n",
    "\n",
    "s = \"\"\n",
    "s = s.join(words)\n",
    "\n",
    "ngrams = [s[i : i + n] for n in range(1, 8) for i in range(len(s) - n + 1)]\n",
    "#ngrams from unigrams up to 7-grams\n",
    "\n",
    "# print(ngrams)\n",
    "\n",
    "dictall = Counter(ngrams)\n",
    "\n",
    "\n",
    "frequency = dictall.__sizeof__()\n",
    "\n",
    "\n",
    "TestWord = \"fictional\"\n",
    "\n",
    "\n",
    "\n",
    "if TestWord in words:\n",
    "    print(\"Found it!!,word is correct\")\n",
    "\n",
    "elif TestWord in dictall:\n",
    "\n",
    "    probability = dictall[TestWord] / frequency\n",
    "\n",
    "    print(\"Probability of {} is : {} \".format(TestWord, probability))\n",
    "\n",
    "else:\n",
    "\n",
    "    for n in range(1, len(TestWord)):\n",
    "\n",
    "        if TestWord[:-n] in dictall:\n",
    "\n",
    "            smoothing = 1\n",
    "            probability = (dictall[TestWord[:-n]] + smoothing) / frequency\n",
    "\n",
    "            print(\"Probability of {} is : {} \".format(TestWord[:-n], probability))\n",
    "\n",
    "        else:\n",
    "            print(\"Nothing is found  at {} \".format(TestWord[:-n]))\n",
    "\n",
    "'''\n",
    "Output : \n",
    "\n",
    "Nothing is found  at fictiona \n",
    "Probability of fiction is : 2.7109086965950985e-05 \n",
    "Probability of fictio is : 2.7109086965950985e-05 \n",
    "Probability of ficti is : 2.7109086965950985e-05 \n",
    "Probability of fict is : 2.7109086965950985e-05 \n",
    "Probability of fic is : 2.7109086965950985e-05 \n",
    "Probability of fi is : 4.744090219041423e-05 \n",
    "Probability of f is : 0.0001152136196052917 \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Code block, I have used an N-gram language model which calculates the N-gram of the corpus and then compares it with the word given.  \n",
    "It tries to produce the probability of N-grams for the given word starting from the last word and reducting 1 letter.  \n",
    "  \n",
    "This way, We can use the N-grams to predict the next letter present or should be present in the word using the probability of that N-gram.  \n",
    "  \n",
    "(Note : I have used a Test string 'Contents' to show the working of N-gram and not corpus due to limited configuration of my system)  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
